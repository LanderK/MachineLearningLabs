{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2:  Decision trees\n",
    "The following exercise takes you through for implementing decision trees. It involves data manipulation/visualisation, hyperparameter selection, recursion, and building a prediction model. We will use a binary classification problem: Breast cancer diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking and Submission\n",
    "\n",
    "These lab exercises are marked, and contribute to your final grade. For this lab exercise there are 6 places where you are expected to enter your own code, for 15 marks overall. Every place you have to add code is indicated by\n",
    "\n",
    "`# **************************************************************** n marks`\n",
    "\n",
    "with instructions above the code block.\n",
    "\n",
    "Please submit your completed workbook using Moodle before 5pm on the 1st November 2017. The workbook you submit must be an `.ipynb` file, which is saved into the directory you're running Jupyter; alternatively you can download it from the menu above using `File -> Download As -> Notebook (.ipynb)`. Remember to save your work regularly (Save and checkpoint in the File menu, the icon of a floppy disk, or Ctrl-S); the version you submit should have all code blocks showing the results (if any) of execution below them. You will normally receive feedback within a week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets as ds\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "The first step of any machine learning problem is to load the data. In this tutorial you don't have to download any dataset since we are using a built-in dataset provided by the scikit learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_all = ds.load_breast_cancer()\n",
    "\n",
    "x = data_all.data\n",
    "y = data_all.target\n",
    "\n",
    "y_names = data_all.target_names \n",
    "\n",
    "feature_names = data_all.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Wisconsin (Diagnostic) Database\n",
    "A description of the dataset used is provided here.\n",
    "\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 569\n",
    "\n",
    "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
    "\n",
    "    :Attribute Information:\n",
    "        - radius (mean of distances from center to points on the perimeter)\n",
    "        - texture (standard deviation of gray-scale values)\n",
    "        - perimeter\n",
    "        - area\n",
    "        - smoothness (local variation in radius lengths)\n",
    "        - compactness (perimeter^2 / area - 1.0)\n",
    "        - concavity (severity of concave portions of the contour)\n",
    "        - concave points (number of concave portions of the contour)\n",
    "        - symmetry \n",
    "        - fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "        largest values) of these features were computed for each image,\n",
    "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "        13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "        - target class:\n",
    "                - WDBC-Malignant\n",
    "                - WDBC-Benign\n",
    "\n",
    "    :Summary Statistics:\n",
    "\n",
    "    ===================================== ====== ======\n",
    "                                           Min    Max\n",
    "    ===================================== ====== ======\n",
    "    radius (mean):                        6.981  28.11\n",
    "    texture (mean):                       9.71   39.28\n",
    "    perimeter (mean):                     43.79  188.5\n",
    "    area (mean):                          143.5  2501.0\n",
    "    smoothness (mean):                    0.053  0.163\n",
    "    compactness (mean):                   0.019  0.345\n",
    "    concavity (mean):                     0.0    0.427\n",
    "    concave points (mean):                0.0    0.201\n",
    "    symmetry (mean):                      0.106  0.304\n",
    "    fractal dimension (mean):             0.05   0.097\n",
    "    radius (standard error):              0.112  2.873\n",
    "    texture (standard error):             0.36   4.885\n",
    "    perimeter (standard error):           0.757  21.98\n",
    "    area (standard error):                6.802  542.2\n",
    "    smoothness (standard error):          0.002  0.031\n",
    "    compactness (standard error):         0.002  0.135\n",
    "    concavity (standard error):           0.0    0.396\n",
    "    concave points (standard error):      0.0    0.053\n",
    "    symmetry (standard error):            0.008  0.079\n",
    "    fractal dimension (standard error):   0.001  0.03\n",
    "    radius (worst):                       7.93   36.04\n",
    "    texture (worst):                      12.02  49.54\n",
    "    perimeter (worst):                    50.41  251.2\n",
    "    area (worst):                         185.2  4254.0\n",
    "    smoothness (worst):                   0.071  0.223\n",
    "    compactness (worst):                  0.027  1.058\n",
    "    concavity (worst):                    0.0    1.252\n",
    "    concave points (worst):               0.0    0.291\n",
    "    symmetry (worst):                     0.156  0.664\n",
    "    fractal dimension (worst):            0.055  0.208\n",
    "    ===================================== ====== ======\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
    "\n",
    "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
    "\n",
    "    :Donor: Nick Street\n",
    "\n",
    "    :Date: November, 1995\n",
    "\n",
    "This is a copy of the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset from https://goo.gl/U2Uwz2\n",
    "\n",
    "Features are computed from a digitized image of a fine needle\n",
    "aspirate (FNA) of a breast mass. They describe\n",
    "characteristics of the cell nuclei present in the image.\n",
    "\n",
    "Separating plane described above was obtained using\n",
    "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "Construction Via Linear Programming.\" Proceedings of the 4th\n",
    "Midwest Artificial Intelligence and Cognitive Science Society,\n",
    "pp. 97-101, 1992], a classification method which uses linear\n",
    "programming to construct a decision tree.  Relevant features\n",
    "were selected using an exhaustive search in the space of 1-4\n",
    "features and 1-3 separating planes.\n",
    "\n",
    "The actual linear program used to obtain the separating plane\n",
    "in the 3-dimensional space is that described in:\n",
    "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
    "Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "This database is also available through the UW CS ftp server:\n",
    "\n",
    "```\n",
    "ftp ftp.cs.wisc.edu\n",
    "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "```\n",
    "\n",
    "### References\n",
    "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
    "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
    "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
    "     San Jose, CA, 1993.\n",
    "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
    "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
    "     July-August 1995.\n",
    "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
    "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
    "     163-171.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare/Split data\n",
    "We provide the data preparation part. The bellow code block splits the data and the targets into training and test sets; 60% for training, 40% for test. This repartition is of course arbitrary, different percentages could be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 341\n",
      "Test set size: 228\n"
     ]
    }
   ],
   "source": [
    "split = int(x.shape[0] * 0.6)\n",
    "\n",
    "x_train = x[:split,:]\n",
    "y_train = y[:split]\n",
    "\n",
    "x_test = x[split:,:]\n",
    "y_test = y[split:]\n",
    "\n",
    "print('Training set size:', x_train.shape[0])\n",
    "print('Test set size:', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation\n",
    "\n",
    "Since our data has a feature dimensionality of 30, it is difficult for us to visualise it. We visualize data by using a dimensionality reduction technique called Principal Component Analysis (PCA). \n",
    "\n",
    "Given an array in `R^{nxd}` (a matrix of size `n X d` with real entries) with `n` and `d` being the number of data points and the feature dimensionality, respectively, PCA will output an array in `R^{nxm}`, with `m<d`. \n",
    "\n",
    "PCA will be covered in future lectures. But for now, you can consider it as a way to reduce the dimensionality of our feature space. \n",
    "\n",
    "In order to be able to visualise the data on a 2D plot, we choose `m=2` (`m=3` is also a possibility)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1  (1 mark): \n",
    "\n",
    "Complete the code block below by writing the code that plots the reduced data obtained with PCA. Use different colours and markers to distinguish between positive and negative samples.\n",
    "\n",
    "It is interesting and informative to see how the result looks like if we remove the scaling part. \n",
    "\n",
    "The results should look like the plot below (please note that this is a plot of another dataset):\n",
    "\n",
    "<img src=\"image.png\">\n",
    "\n",
    "Hint: You will need to google the documentation for the `scatter()` and `legend()` methods of matplotlib.\n",
    "\n",
    "__(1 mark)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYRgtiikAwzCMFsUUgGEYRovy/wAvnSRGbbRybQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2841ff70b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "x_scaled = preprocessing.scale(x[:,:-1]) # We remove the indexing and make sure all the features are in N(0,1)\n",
    "x_reduced = pca.fit_transform(x_scaled)\n",
    "#x_reduced = pca.fit_transform(x[:,0:-1]) # Uncomment this to see the result without scaling\n",
    "\n",
    "# **************************************************************** 1 mark\n",
    "\n",
    "plt.scatter(x_reduced[y==0,0], x_reduced[y==0,1], c='b', marker='+',label=\"Negative\") \n",
    "plt.scatter(x_reduced[y==1,0], x_reduced[y==1,1], c='r', marker='.',label=\"Positive\") \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2  (4 marks): \n",
    "Complete the function `calculate_entropy(y)` in the code block bellow. The input is a column vector of target class values, and the output is its entropy.\n",
    "\n",
    "`y` is an `n X 1` sized matrix where `n` is the number of data points (targets).\n",
    "The return is a scalar.\n",
    "\n",
    "Hints:\n",
    " * You may want to google the documentation for `numpy.unique()`,  paying particular attention to the `return_counts` keyword.\n",
    " * Be careful about type - you may need to use `.astype(float)` to avoid integer division.\n",
    "\n",
    "__(4 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    # **************************************************************** 4 marks\n",
    "    #Calculate the probabilities for Positive and Negative outcomes\n",
    "    counts = np.unique(y,return_counts=True)\n",
    "    probs = counts[1].astype(float) / y.shape[0]\n",
    "    #Calculate the Etropy from the probabilities\n",
    "    entropy = -(probs * np.log2(probs)).sum()\n",
    "    return entropy\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3  (5 marks): \n",
    "Use the function `calculate_entropy()` to complete the function `find_split(x, y)`.\n",
    "\n",
    "`find_split(x, y)` takes as input:\n",
    " * The data matrix of features, `x` in `R^{nXd}`. `n` is the number of data points and `d` is the feature dimensionality. \n",
    " * `y`, a column vector of size `n` containing the target value for each data point in `x`.\n",
    "\n",
    "`find_split(x, y)` outputs 'best_split' which is a dictionary (see the last part of the below code) with the following keys and their corresponding values:\n",
    "\n",
    " * `'feature'`: An integer indexing the attribute/feature chosen to split upon.\n",
    " * `'split'`: The value/threshold of this feature to split at.\n",
    " * `'infogain'`: A scalar representing the amount of information gained by splitting this way.\n",
    " * `'left_indices'`: Indices of the exemplars that satisfy `x[feature_index]<=split`.\n",
    " * `'right_indices'`: Opposite set of indices to `left_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_split(x, y):\n",
    "    \"\"\"Given a dataset and its target values, this finds the optimal combination\n",
    "    of feature and split point that gives the maximum information gain.\"\"\"\n",
    "    \n",
    "    # Need the starting entropy so we can measure improvement...\n",
    "    start_entropy = calculate_entropy(y)\n",
    "    \n",
    "    # Best thus far, initialised to a dud that will be replaced immediately...\n",
    "    best = {'infogain' : -np.inf}\n",
    "    \n",
    "    # Loop every possible split of every dimension...\n",
    "    for i in range(x.shape[1]):\n",
    "        for split in np.unique(x[:,i]):\n",
    "            \n",
    "            # Find the indices for the data which pass/fail the feature split\n",
    "            left_indices  = np.where(x[:,i]<=split)[0]\n",
    "            right_indices = np.where(x[:,i]>split)[0]\n",
    "            \n",
    "            # Get the entropies of the split data sets \n",
    "            left_ent = calculate_entropy(y[left_indices])\n",
    "            right_ent = calculate_entropy(y[right_indices])\n",
    "            \n",
    "            left_prob = left_indices.shape[0] / y.shape[0]\n",
    "            right_prob = right_indices.shape[0] / y.shape[0]\n",
    "            \n",
    "            infogain = start_entropy - (left_prob*left_ent +right_prob*right_ent)\n",
    "            \n",
    "            # **************************************************************** 5 marks\n",
    "            \n",
    "            if infogain > best['infogain']:\n",
    "                best = {'feature' : i,\n",
    "                        'split' : split,\n",
    "                        'infogain' : infogain, \n",
    "                        'left_indices' : left_indices,\n",
    "                        'right_indices' : right_indices}\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function `find_split()` allows us to find the optimal feature and the best value to split the data into two chunks. Applying this to the original data set splits it into two new data sets. We can then repeat this on both of the new data sets to get four data sets, and so on. This recursion builds a decision tree. It needs a stopping condition, to prevent it dividing the data forever, here we will use two:\n",
    " * Maximum depth: The tree is limited to be no deeper than a provided limit.\n",
    " * Perfection: If a node contains only one class then there is little point in splitting it further.\n",
    "\n",
    "We provide the function `build_tree(x, y, max_depth)` below to construct a tree. The inputs are: \n",
    "\n",
    " * The data matrix of features, `x` in `R^{nXd}`. `n` is the number of data points and `d` is the feature dimensionality. \n",
    " * `y`, a column vector of size `n` containing the target value for each data point in `x`.\n",
    " * The maximum depth of the tree, `max_depth`.\n",
    "\n",
    "The output of this function is a dictionary. If it has generated a leaf node then the keys are:\n",
    " * `'leaf' : True`\n",
    " * `'class'` : The index of the class to assign to exemplars that land here.\n",
    "\n",
    "If it has generated a split node then the keys are:\n",
    " * `'leaf' : False`\n",
    " * `'feature'`: The feature to apply the `split` to.\n",
    " * `'split'`: The split to test the exemplars `feature` with.\n",
    " * `'infogain'`: The information gain of this split.\n",
    " * `'left'` : The left subtree, for exemplars where `x[feature_index]<=split`\n",
    " * `'right'` : The right subtree, for exemplars where `x[feature_index]>split`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_tree(x, y, max_depth = np.inf):\n",
    "    # Check if either of the stopping conditions have been reached. If so generate a leaf node...\n",
    "    if max_depth==1 or (y==y[0]).all():\n",
    "        # Generate a leaf node...\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        return {'leaf' : True, 'class' : classes[np.argmax(counts)]}\n",
    "    \n",
    "    else:\n",
    "        move = find_split(x, y)\n",
    "        \n",
    "        left = build_tree(x[move['left_indices'],:], y[move['left_indices']], max_depth - 1)\n",
    "        right = build_tree(x[move['right_indices'],:], y[move['right_indices']], max_depth - 1)\n",
    "        \n",
    "        return {'leaf' : False,\n",
    "                'feature' : move['feature'],\n",
    "                'split' : move['split'],\n",
    "                'infogain' : move['infogain'],\n",
    "                'left' : left,\n",
    "                'right' : right}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After building the tree we should be able to predict the class of a sample. We do that by propagating the sample through the tree, i.e. we check all the splitting conditions until the sample falls in a leaf node, in which case the class of the leaf node is attributed to the sample.\n",
    "\n",
    "We provide the recursive function `predict_one(tree, sample)` that takes as input the constructed tree, a sample in `R^d` and recursively propagates it through the branches of our tree. The output of this function is the class predicted for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_one(tree, sample):\n",
    "    \"\"\"Does the prediction for a single data point\"\"\"\n",
    "    if tree['leaf']:\n",
    "        return tree['class']\n",
    "    \n",
    "    else:\n",
    "        if sample[tree['feature']] <= tree['split']:\n",
    "            return predict_one(tree['left'], sample)\n",
    "        else:\n",
    "            return predict_one(tree['right'], sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further generalize the prediction function above to the case where we have a data matrix `R^{nXd}` representing many data points. the function `predict(tree, samples)` bellow takes as input the constructed tree and a data array then returns an array containing the predictions for all the samples in our input data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(tree, samples):\n",
    "    \"\"\"Predicts class for every entry of a data matrix.\"\"\"\n",
    "    ret = np.empty(samples.shape[0], dtype=int)\n",
    "    ret.fill(-1)\n",
    "    indices = np.arange(samples.shape[0])\n",
    "    \n",
    "    def tranverse(node, indices):\n",
    "        nonlocal samples\n",
    "        nonlocal ret\n",
    "        \n",
    "        if node['leaf']:\n",
    "            ret[indices] = node['class']\n",
    "        \n",
    "        else:\n",
    "            going_left = samples[indices, node['feature']] <= node['split']\n",
    "            left_indices = indices[going_left]\n",
    "            right_indices = indices[np.logical_not(going_left)]\n",
    "            \n",
    "            if left_indices.shape[0] > 0:\n",
    "                tranverse(node['left'], left_indices)\n",
    "                \n",
    "            if right_indices.shape[0] > 0:\n",
    "                tranverse(node['right'], right_indices)\n",
    "    \n",
    "    tranverse(tree, indices)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4  (2 marks):\n",
    "Using the functions defined above build a tree and report the training and test accuracy.\n",
    "\n",
    "__(2 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training result = 100.0 %\n",
      "testing result = 89.9122807018 %\n"
     ]
    }
   ],
   "source": [
    "# **************************************************************** 2 marks\n",
    "# Build Tree on the Training Data\n",
    "tree = build_tree(x_train,y_train)\n",
    "\n",
    "# Get the Training Accuracy\n",
    "train_results =  predict(tree , x_train)\n",
    "train_ac = (train_results==y_train).sum()/ y_train.shape[0]\n",
    "\n",
    "print ('training result =', train_ac*100 ,'%')\n",
    "\n",
    "# Get the Testing Accuracy\n",
    "test_results =  predict(tree , x_test)\n",
    "test_ac = (test_results==y_test).sum()/ y_test.shape[0]\n",
    "\n",
    "print ('testing result =', test_ac*100 , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (2 marks):\n",
    "\n",
    "Find the best `max_depth` parameter plus its corresponding training and test accuracies. A good range to test is `range(2,6)`.\n",
    "\n",
    "__(2 marks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying depth: 2\n",
      "Trying depth: 3\n",
      "Trying depth: 4\n",
      "Trying depth: 5\n",
      "\n",
      "Best tree has\n",
      "Depth: 4\n",
      "Training Accuracy: 0.973607038123\n",
      "Testing Accuracy: 0.916666666667\n"
     ]
    }
   ],
   "source": [
    "# **************************************************************** 2 marks\n",
    "best_depth = None\n",
    "best_tree = None\n",
    "best_train = -0.1\n",
    "best_test = -0.1\n",
    "\n",
    "# Loop over every tree of depth 2 to 6 and choose the one that has the best Testing Accuracy \n",
    "for d in range(2,6):\n",
    "    print('Trying depth:' ,d)\n",
    "    tree= build_tree(x_train, y_train, d)\n",
    "    \n",
    "    train_results =  predict(tree , x_train)\n",
    "    train_ac = (train_results==y_train).sum()/ y_train.shape[0]\n",
    "\n",
    "    test_results =  predict(tree , x_test)\n",
    "    test_ac = (test_results==y_test).sum()/ y_test.shape[0]\n",
    "    \n",
    "    if test_ac > best_test:\n",
    "        best_depth = d\n",
    "        best_tree = tree\n",
    "        best_train = train_ac\n",
    "        best_test = test_ac\n",
    "        \n",
    "print('')\n",
    "        \n",
    "print('Best tree has')\n",
    "print('Depth:', best_depth)\n",
    "print('Training Accuracy:', best_train)\n",
    "print('Testing Accuracy:', best_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (1 mark):\n",
    "Write a recursive function that prints out a tree, and use it to print the best tree learned.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "    [x22 <=0.296]\n",
    "\n",
    "        [x27 <=-0.058]\n",
    "\n",
    "          [x13 <=0.187]\n",
    "\n",
    "           [x21 <=1.246]\n",
    "                predict 1\n",
    "                \n",
    "           [x21 >1.246]\n",
    "                predict 0\n",
    "                \n",
    "          [x13 >0.187]\n",
    "\n",
    "           [x0 <=0.160]\n",
    "                predict 1\n",
    "                \n",
    "           [x0 >0.160]\n",
    "                predict 0\n",
    "                \n",
    "        [x27 >-0.058]\n",
    "\n",
    "          [x27 <=0.690]\n",
    "\n",
    "           [x21 <=0.263]\n",
    "            predict 1\n",
    "            \n",
    "           [x21 >0.263]\n",
    "            predict 0\n",
    "            \n",
    "          [x27 >0.690]\n",
    "            predict 0\n",
    "            \n",
    "    [x22 >0.296]\n",
    "\n",
    "    predict 0\n",
    "```\n",
    "\n",
    "The conditions with the same tree depth must be indented the same amount. This function should have as input the tree learned, and a scalar `indent` that is used to measure how for to indent at the current recursion level.\n",
    "\n",
    "__(1 mark)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[x22 <= 105.0]\n",
      "    [x24 <= 0.1733]\n",
      "        [x21 <= 23.31]\n",
      "             predict 1\n",
      "\n",
      "        [x21 > 23.31]\n",
      "             predict 1\n",
      "\n",
      "\n",
      "    [x24 > 0.1733]\n",
      "         predict 0\n",
      "\n",
      "\n",
      "[x22 > 105.0]\n",
      "    [x22 <= 114.3]\n",
      "        [x1 <= 19.65]\n",
      "             predict 1\n",
      "\n",
      "        [x1 > 19.65]\n",
      "             predict 0\n",
      "\n",
      "\n",
      "    [x22 > 114.3]\n",
      "        [x7 <= 0.02771]\n",
      "             predict 1\n",
      "\n",
      "        [x7 > 0.02771]\n",
      "             predict 0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tree(tree, indent = 0):\n",
    "    # **************************************************************** 1 mark\n",
    "    if tree['leaf']:\n",
    "        print(' '*indent, 'predict', tree['class'])\n",
    "    else:\n",
    "        \n",
    "        print(' '*indent, '[x' , tree['feature'],' <= ', tree['split'],']', sep='')\n",
    "        print_tree(tree['left'], indent+4)\n",
    "        print('')\n",
    "        \n",
    "        print(' '*indent, '[x' , tree['feature'],' > ', tree['split'],']', sep='')\n",
    "        print_tree(tree['right'], indent+4)\n",
    "        print('')\n",
    "        \n",
    "\n",
    "print_tree(best_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
